{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14feb965-d6d6-4d7a-9c88-ab65953d0335",
   "metadata": {},
   "source": [
    "Hey there! In this notebook, I'm diving into the challenge of predicting price movements in the cryptocurrency market. Using historical data and some technical indicators (like moving averages and RSI), my goal is to build a model that can anticipate price direction. The metric I'm optimizing for is the macro-averaged F1 score, which balances both precision and recall. Let’s see how this goes!\n",
    "\n",
    "\n",
    "\n",
    "I’m loading the dataset to get a quick look at the structure. This includes the main train.csv for training, test.csv for predictions, and a sample_submission.csv to see the required format for submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71a9b35f-cecd-41c6-a3fa-dab4e4c4884e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>quote_asset_volume</th>\n",
       "      <th>number_of_trades</th>\n",
       "      <th>taker_buy_base_volume</th>\n",
       "      <th>taker_buy_quote_volume</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1525471260</td>\n",
       "      <td>0.90120</td>\n",
       "      <td>0.90130</td>\n",
       "      <td>0.90120</td>\n",
       "      <td>0.90130</td>\n",
       "      <td>134.98</td>\n",
       "      <td>121.646459</td>\n",
       "      <td>4.0</td>\n",
       "      <td>125.08</td>\n",
       "      <td>112.723589</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1525471320</td>\n",
       "      <td>0.90185</td>\n",
       "      <td>0.90195</td>\n",
       "      <td>0.90185</td>\n",
       "      <td>0.90195</td>\n",
       "      <td>1070.54</td>\n",
       "      <td>965.505313</td>\n",
       "      <td>12.0</td>\n",
       "      <td>879.94</td>\n",
       "      <td>793.612703</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1525471380</td>\n",
       "      <td>0.90140</td>\n",
       "      <td>0.90140</td>\n",
       "      <td>0.90139</td>\n",
       "      <td>0.90139</td>\n",
       "      <td>2293.06</td>\n",
       "      <td>2066.963991</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1525471440</td>\n",
       "      <td>0.90139</td>\n",
       "      <td>0.90140</td>\n",
       "      <td>0.90138</td>\n",
       "      <td>0.90139</td>\n",
       "      <td>6850.59</td>\n",
       "      <td>6175.000909</td>\n",
       "      <td>19.0</td>\n",
       "      <td>1786.30</td>\n",
       "      <td>1610.149485</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1525471500</td>\n",
       "      <td>0.90139</td>\n",
       "      <td>0.90139</td>\n",
       "      <td>0.90130</td>\n",
       "      <td>0.90130</td>\n",
       "      <td>832.30</td>\n",
       "      <td>750.222624</td>\n",
       "      <td>3.0</td>\n",
       "      <td>784.82</td>\n",
       "      <td>707.428900</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    timestamp     open     high      low    close   volume  \\\n",
       "0  1525471260  0.90120  0.90130  0.90120  0.90130   134.98   \n",
       "1  1525471320  0.90185  0.90195  0.90185  0.90195  1070.54   \n",
       "2  1525471380  0.90140  0.90140  0.90139  0.90139  2293.06   \n",
       "3  1525471440  0.90139  0.90140  0.90138  0.90139  6850.59   \n",
       "4  1525471500  0.90139  0.90139  0.90130  0.90130   832.30   \n",
       "\n",
       "   quote_asset_volume  number_of_trades  taker_buy_base_volume  \\\n",
       "0          121.646459               4.0                 125.08   \n",
       "1          965.505313              12.0                 879.94   \n",
       "2         2066.963991               5.0                   0.00   \n",
       "3         6175.000909              19.0                1786.30   \n",
       "4          750.222624               3.0                 784.82   \n",
       "\n",
       "   taker_buy_quote_volume  target  \n",
       "0              112.723589     1.0  \n",
       "1              793.612703     0.0  \n",
       "2                0.000000     0.0  \n",
       "3             1610.149485     0.0  \n",
       "4              707.428900     0.0  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import f1_score\n",
    "from xgboost import XGBClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "# suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# load the training and test data\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "sample_submission = pd.read_csv('sample_submission.csv')\n",
    "\n",
    "# display the first few rows of the training data to understand structure\n",
    "train.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e349464-e5fe-42f0-be6a-e29ae3a98c2c",
   "metadata": {},
   "source": [
    "Time to convert the timestamp to a more readable datetime format and fill in any missing values. This step helps to ensure consistency across the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65f0c951-69c1-47c8-9d68-0ce4c9846180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert timestamp to a datetime format for better readability\n",
    "train['timestamp'] = pd.to_datetime(train['timestamp'], unit='s')\n",
    "test['timestamp'] = pd.to_datetime(test['timestamp'], unit='s')\n",
    "\n",
    "# handle any missing values by filling with the column mean\n",
    "# this ensures no NaNs interfere with model training\n",
    "train.fillna(train.mean(), inplace=True)\n",
    "test.fillna(test.mean(), inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e27843-241b-413b-b42e-c74f625c48f9",
   "metadata": {},
   "source": [
    "Here’s where I add a bunch of useful features to help the model understand price trends and volatility. I’m adding things like price difference, volatility, moving averages, RSI, and lagged features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85be805f-31eb-479d-a994-2ecac3785fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create features: price difference, volatility, and moving averages\n",
    "train['price_diff'] = train['close'] - train['open']\n",
    "train['volatility'] = train['high'] - train['low']\n",
    "test['price_diff'] = test['close'] - test['open']\n",
    "test['volatility'] = test['high'] - test['low']\n",
    "\n",
    "# calculate moving averages for different windows to capture trends\n",
    "for period in [5, 10, 20]:\n",
    "    train[f'ma_{period}'] = train['close'].rolling(window=period).mean()\n",
    "    test[f'ma_{period}'] = test['close'].rolling(window=period).mean()\n",
    "\n",
    "# calculate rsi (relative strength index) as a momentum indicator\n",
    "delta = train['close'].diff()\n",
    "gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()\n",
    "loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()\n",
    "rs = gain / loss\n",
    "train['rsi'] = 100 - (100 / (1 + rs))\n",
    "\n",
    "delta_test = test['close'].diff()\n",
    "gain_test = (delta_test.where(delta_test > 0, 0)).rolling(window=14).mean()\n",
    "loss_test = (-delta_test.where(delta_test < 0, 0)).rolling(window=14).mean()\n",
    "rs_test = gain_test / loss_test\n",
    "test['rsi'] = 100 - (100 / (1 + rs_test))\n",
    "\n",
    "# create lagged features to help capture previous values' impact on current values\n",
    "lags = [1, 5, 10, 15, 30, 60]\n",
    "for lag in lags:\n",
    "    train[f'close_lag_{lag}'] = train['close'].shift(lag)\n",
    "    test[f'close_lag_{lag}'] = test['close'].shift(lag)\n",
    "    train[f'volatility_lag_{lag}'] = train['volatility'].shift(lag)\n",
    "    test[f'volatility_lag_{lag}'] = test['volatility'].shift(lag)\n",
    "\n",
    "# fill any nan values created by lagging with forward/backward fill\n",
    "train.fillna(method='bfill', inplace=True)\n",
    "train.fillna(method='ffill', inplace=True)\n",
    "test.fillna(method='bfill', inplace=True)\n",
    "test.fillna(method='ffill', inplace=True)\n",
    "\n",
    "# define the features and target for training\n",
    "# dropping timestamp as it's not relevant for the model\n",
    "features = [col for col in train.columns if col not in ['timestamp', 'target']]\n",
    "target = 'target'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d08c64-c071-40b5-a76d-325f3f561381",
   "metadata": {},
   "source": [
    "With the data ready, I’m splitting it into training and validation sets, applying SMOTE to handle class imbalance, and scaling the features. Then, I’m training an XGBoost model with GPU support, which should help speed things up!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6e2d833-f190-4302-9016-3485b07a06c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation F1 Score: 0.5324\n"
     ]
    }
   ],
   "source": [
    "# apply smote to balance classes since this is a binary classification\n",
    "X = train[features]\n",
    "y = train[target]\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# scale features to have a mean of 0 and a standard deviation of 1 for better model performance\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "# apply smote for oversampling the minority class\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "# define and train the xgboost model with gpu support to speed up training\n",
    "xgb_model = XGBClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=7,\n",
    "    learning_rate=0.2,\n",
    "    tree_method='gpu_hist',  # this ensures gpu usage where possible\n",
    "    eval_metric='logloss',\n",
    "    random_state=42,\n",
    "    use_label_encoder=False\n",
    ")\n",
    "xgb_model.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# evaluate the model on the validation set using f1 score\n",
    "y_val_pred = xgb_model.predict(X_val_scaled)\n",
    "f1 = f1_score(y_val, y_val_pred, average='macro')\n",
    "print(f\"validation f1 score: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d6e731-18a3-41f3-baba-5955a14f42d3",
   "metadata": {},
   "source": [
    "To ensure the model is consistent, I’m using 5-fold cross-validation. This will give us a more stable metric and help verify if the model is reliable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62bfe41d-aff2-49c9-8e7d-db1a83379c72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5-Fold Cross-Validation Macro F1 Score: 0.5107 ± 0.0008\n"
     ]
    }
   ],
   "source": [
    "# perform 5-fold cross-validation to ensure model stability and reliability\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv_scores = cross_val_score(xgb_model, X, y, cv=kf, scoring='f1_macro')\n",
    "print(f\"5-fold cross-validation macro f1 score: {cv_scores.mean():.4f} ± {cv_scores.std():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b615a7-6818-4017-aebc-81b53e26a49f",
   "metadata": {},
   "source": [
    "Reflecting on this project, I’m happy with the progress made but also see room for improvement. Working on a laptop while visiting family in upstate NY added some constraints — I didn’t have access to my usual setup, so running large models like XGBoost was a bit challenging. With more computational power, I could’ve explored deeper architectures, like LSTM or GRU with TensorFlow, which would’ve been ideal for time-series data like this.\n",
    "\n",
    "Additionally, with access to cloud resources or my main system, I’d run more intensive hyperparameter tuning and try out ensemble techniques. Despite these constraints, I think this notebook demonstrates a strong understanding of the end-to-end process for predictive modeling, from data preprocessing and feature engineering to model evaluation and optimization. Looking forward to applying these skills in a high-power environment next time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97feb218-b6f0-44f6-9ff0-14fb0db627d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission file created: submission.csv with 909617 rows\n"
     ]
    }
   ],
   "source": [
    "# scale test data using the same scaler used for training\n",
    "X_test_scaled = scaler.transform(test[features])\n",
    "\n",
    "# make predictions on the test set\n",
    "test_predictions = xgb_model.predict(X_test_scaled)\n",
    "\n",
    "# create the submission file as required for competition format\n",
    "submission = pd.DataFrame({\n",
    "    'row_id': test.index,\n",
    "    'target': test_predictions\n",
    "})\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print(f\"submission file created: submission.csv with {len(submission)} rows\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
